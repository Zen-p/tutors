## Docker, контейнеры и продакшн‑окружения

Современные продакшн‑системы практически всегда опираются на контейнеризацию. Docker решает сразу несколько проблем: стандартная упаковка артефактов, одинаковое окружение для девелопера, CI и продакшна, быстрый запуск множества инстансов сервисов и изоляция зависимостей. Контейнер — это не просто «лёгкая виртуалка», а воспроизводимое окружение с чётко описанным образом, версионируемым так же, как и код.

Ключ к здоровой продакшн‑инфраструктуре — в правильной организации Docker‑образов и их жизненного цикла. Базовая практика заключается в том, чтобы исходный код вообще не деплоился напрямую на серверы. Вместо этого на CI сервере для каждого билда создаётся Docker‑образ с вашей Java‑аппликацией, этот образ пушится в реестр (например, Docker Hub), а все среды (стейджинг, продакшн) запускают только проверенные теги образов.

При создании Docker‑образов для Java‑микросервисов важно следовать нескольким практикам, которые хорошо видны в ваших заметках. Во‑первых, нужно использовать воспроизводимые команды сборки, например комбинацию `mvn clean package` или профиль `docker-build-image`, чтобы каждый новый билд не зависел от артефактов прошлых запусков. Во‑вторых, контейнеры должны быть максимально независимыми от хостовой системы: все зависимости, включая JDK, библиотеки и конфигурацию, описываются внутри образа. В‑третьих, желательно использовать многоэтапные сборки или специализированные плагины (например, Jib), чтобы не тащить в финальный образ весь Maven и исходники.

Практика разделения ответственности на уровне контейнеров означает, что один контейнер отвечает за один процесс. Для микросервисной системы это означает отдельные контейнеры для каждого Java‑сервиса, для базы данных (PostgreSQL), брокера сообщений (RabbitMQ), системы трассировки (Zipkin), сервис‑реестра (Eureka) и т. д. Такая декомпозиция делает систему проще в мониторинге и масштабировании: вы можете отдельно поднять ещё несколько инстансов сервиса, не трогая базу данных, и наоборот. Без Docker‑подхода подобная гибкость была бы гораздо сложнее.

### Dockerfile для Java‑сервиса и многоэтапная сборка

В ваших заметках приведён пример более классического подхода, когда внутри Dockerfile ещё и устанавливается Maven. Для продакшна на практике чаще используют многоэтапные сборки или Jib, но полезно понять, как выглядит базовый Dockerfile.

```dockerfile
FROM eclipse-temurin:21-jdk-alpine

WORKDIR /app

COPY . .

RUN ./mvnw clean package

ENTRYPOINT ["java", "-jar", "target/your-app-name.jar"]
```

Этот Dockerfile показывает самый прямолинейный путь. Сначала выбирается базовый образ с JDK 21 на базе Alpine, далее рабочая директория устанавливается в `/app`, весь проект копируется внутрь контейнера, Maven выполняет `clean package`, а затем точкой входа объявляется запуск готового JAR‑файла. В реальных продакшн‑системах такой Dockerfile обычно дорабатывают: либо выносят фазу сборки в отдельный «build stage», после чего во второй стадии используют более лёгкий runtime‑образ, либо вообще делегируют построение образа Maven‑плагину Jib, чтобы Dockerfile был не нужен. Но даже в этом виде Dockerfile фиксирует один важный принцип: билд и рантайм приложения можно полностью описать декларативно.

### .dockerignore и уменьшение контекста сборки

Хотя в самом файле заметок `.dockerignore` явно не показан, он логически продолжает те же идеи оптимизации, которые вы уже используете с Jib и профилями Maven. При сборке Java‑проекта обилие артефактов — `target`, `.git`, временные логи, локальные конфиги разработчика — не должны попадать в контекст сборки Docker. Именно поэтому почти всегда создаётся `.dockerignore` с исключениями для каталогов сборки и локальных артефактов. Это ускоряет сборку образа, уменьшает его размер и исключает попадание секретов в контейнер.

### Один процесс на контейнер и запуск от непривилегированного пользователя

В микросервисной архитектуре особенно важно соблюдать правило «один процесс на контейнер». Ваши заметки демонстрируют отдельные конфигурации для PostgreSQL, pgAdmin, Zipkin, Eureka, RabbitMQ и Java‑микросервисов, что как раз и отражает эту идею: в каждом контейнере живёт чётко определённая ответственность. Отдельно стоит упомянуть вопрос безопасности. В продакшене процессы в контейнере редко запускаются от имени `root`. Вместо этого в Dockerfile создаётся непривилегированный пользователь, и приложение запускается от его имени. Это снижает потенциальный ущерб в случае компрометации одного из микросервисов.

## Docker Compose и многоконтейнерные приложения

Когда речь идёт не об одном сервисе, а о целой микросервисной системе, по отдельности запускать контейнеры неудобно и небезопасно. Docker Compose решает эту задачу, позволяя описать все контейнеры, их зависимости, сети, тома и порты в одном файле. В ваших конспектах есть типичный пример конфигурации для базы данных PostgreSQL и интерфейса управления pgAdmin, который хорошо иллюстрирует этот подход.

```yaml
version: '3.8'

services:
  postgres:
    container_name: postgres
    image: postgres:16.4
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      PGDATA: /data/postgres
    volumes:
      - postgres:/data/postgres
    ports:
      - "5432:5432"
    networks:
      - postgres
    restart: unless-stopped

  pgadmin:
    container_name: pgadmin
    image: dpage/pgadmin4
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admim.com
      PGADMIN_DEFAULT_PASSWORD: admin
    volumes:
      - pgadmin:/var/lib/pgadmin
    ports:
      - "5050:80"
    networks:
      - postgres
    restart: unless-stopped

networks:
  postgres:
    driver: bridge

volumes:
  postgres:
  pgadmin:
```

Эта конфигурация описывает реальный фрагмент инфраструктуры микросервисной системы. Контейнер `postgres` поднимает базу данных версии 16.4, экспортирует порт 5432 и использует именованный Docker‑том `postgres` для хранения данных. Это важно для продакшна, потому что перезапуск контейнера не должен уничтожать данные. Контейнер `pgadmin` даёт удобный веб‑интерфейс для работы с этой базой; он подключён к той же сети `postgres`, что и сам PostgreSQL, и пробрасывает порт 5050 на хост, что соответствует значению из ваших заметок.

В многосервисной системе Docker Compose обычно описывает не только базу данных, но и все остальные инфраструктурные компоненты: Zipkin с портом 9411, Eureka‑сервер на порту 8761, RabbitMQ на 15672, а также все ваши бизнес‑микросервисы. В конспекте упоминаются команды `docker compose down`, `docker compose pull`, `docker compose up -d`, что соответствует типичному рабочему циклу в продакшн‑окружении при деплое новой версии: сначала останавливаются и удаляются запущенные контейнеры, затем подтягиваются обновлённые образы из реестра, после чего весь стек поднимается заново в фоновом режиме.

Отдельно стоит остановиться на директиве `depends_on`, фигурирующей в заметках. Она важна для описания логических зависимостей между сервисами. Например, если `eureka-server` зависит от `zipkin`, это отражается в Compose следующим образом.

```yaml
services:
  zipkin:
    image: openzipkin/zipkin
    ports:
      - "9411:9411"

  eureka-server:
    image: youdzh1n/eureka-server:latest
    container_name: eureka-server
    ports:
      - "8761:8761"
    depends_on:
      - zipkin
```

Семантика здесь следующая: Docker Compose сначала запускает контейнер Zipkin, затем — `eureka-server`. В реальных системах нужно помнить, что `depends_on` не ждёт «полной готовности» сервиса, а только его старт; поэтому для критичных зависимостей дополнительно используют health‑checks и логику повторных подключений внутри самих микросервисов (например, Eureka клиент продолжает пытаться зарегистрироваться, даже если сервер вначале был недоступен).

Для локальной разработки и продакшна зачастую используются разные Spring‑профили. В ваших заметках есть важная деталь: через переменную окружения `SPRING_PROFILES_ACTIVE=docker` можно заставить приложение использовать отдельный конфигурационный файл `application-docker.yml`, содержащий параметры для работы в контейнерах. Этот профиль может включать, например, строку подключения к PostgreSQL по имени сервиса в Compose‑сети вместо `localhost`.

## Построение образов через Jib и Maven

Jib — это Maven‑плагин, который кардинально упрощает создание Docker‑образов для Java‑приложений. В отличие от классического подхода с Dockerfile, Jib интегрируется прямо в Maven‑жизненный цикл и сам разбирает структуру вашего проекта. В ваших заметках есть типичная конфигурация этого плагина в родительском `pom.xml`.

```xml
<plugin>
  <groupId>com.google.cloud.tools</groupId>
  <artifactId>jib-maven-plugin</artifactId>
  <version>3.4.0</version>
  <configuration>
    <from>
      <image>eclipse-temurin:21</image>
      <platforms>
        <platform>
          <architecture>arm64</architecture>
          <os>linux</os>
        </platform>
        <platform>
          <architecture>amd64</architecture>
          <os>linux</os>
        </platform>
      </platforms>
    </from>
    <to>
      <tags>
        <tag>latest</tag>
      </tags>
    </to>
  </configuration>
</plugin>
```

Здесь на уровне родительского проекта задаётся базовый образ `eclipse-temurin:21`, а также платформы `arm64` и `amd64`, что критично, если у вас есть и ARM‑машины (например, MacBook на M‑чипах), и Intel‑серверы. Блок `<to>` отвечает за теги создаваемого образа; по умолчанию указывается `latest`, но в заметках вы также выносите тег в Maven‑свойство `<image>youdzhin/${project.artifactId}:${docker.version.tag}</image>`. Это правильная практика: образ должен быть привязан либо к версии артефакта, либо к отдельному тэгу, который передаётся из CI (например, build‑номер).

На уровне дочернего модуля вы подключаете профиль, который непосредственно активирует Jib для сборки Docker‑образа.

```xml
<profiles>
  <profile>
    <id>docker-build-image</id>
    <build>
      <plugins>
        <plugin>
          <groupId>com.google.cloud.tools</groupId>
          <artifactId>jib-maven-plugin</artifactId>
        </plugin>
      </plugins>
    </build>
  </profile>
</profiles>
```

Когда этот профиль активирован, команда `mvn clean package -P docker-build-image` строит не только JAR, но и контейнерный образ. В заметках также приведён блок `<executions>`, который можно привязать к фазе `package`, чтобы Jib запускался автоматически при каждом пакете. В реальных CI‑конвейерах это используется совместно с GitHub Actions: вы на лету задаёте параметр `-Ddocker.version.tag=...`, и Jib публикует образ в Docker Hub под нужным тегом.

Использование Jib в продакшн‑проектах даёт несколько преимуществ. Во‑первых, нет необходимости поддерживать Dockerfile для каждого микросервиса; вся конфигурация живёт в Maven. Во‑вторых, Jib умеет кэшировать слои и не пересобирать весь образ при каждом изменении кода. В‑третьих, он строит образы без привилегированного доступа к Docker‑демону, что повышает безопасность CI.

## CI/CD с GitHub Actions для бэкенда и Docker

В заметках подробно описаны пайплайны GitHub Actions как для фазы CI (проверка и сборка), так и для CD (сборка и деплой на VPS). Это классический пример продакшн‑цепочки от коммита до развернутого микросервиса.

Ниже приведён типичный workflow для CI, который вы используете для сборки бэкенда с PostgreSQL в качестве сервисной зависимости.

```yaml
name: CI

on:
  push:
  pull_request:

jobs:
  build:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: password
          POSTGRES_DB: pet_project_database
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '21'
          cache: 'maven'
      - name: Build with Maven
        run: mvn -B -ntp package
```

Этот файл описывает, как при каждом пуше или pull request в репозиторий GitHub автоматически разворачивается контейнер с PostgreSQL, настраивается Java 21, и запускается `mvn package`. Опция `-ntp` отключает лишний вывод о прогрессе загрузки артефактов, а кэш Maven (`cache: 'maven'`) существенно ускоряет повторные сборки. В реальном продакшн‑флоу сюда же обычно добавляют запуск интеграционных тестов и линтингов.

Для фазы CD в заметках присутствует отдельный workflow, который не только строит и пушит Docker‑образ через Jib, но и деплоит его на VPS через SSH.

```yaml
name: CD

on:
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: password
          POSTGRES_DB: pet_project_database
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '21'
          cache: 'maven'

      - name: Login to Docker
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_ACCESS_TOKEN }}

      - name: Set build id
        id: build-number
        run: echo "BUILD_NUMBER=$(date '+%d.%m.%Y.%H.%M.%S')" >> $GITHUB_OUTPUT

      - name: Build Package Push with Maven
        run: mvn -ntp -B verify -Ddocker.version.tag=${{steps.build-number.outputs.BUILD_NUMBER}} jib:build

  deploy-to-vps:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to VPS via SSH
        uses: appleboy/ssh-action@v1.0.0
        with:
          host: 5.tcp.eu.ngrok.io:19133
          username: evgenijpesko
          password: zena66
          script: |
            export PATH="/Applications/Docker.app/Contents/Resources/bin:$PATH"
            if [ -d "pet-project" ]; then
              cd pet-project
              git pull origin main
            else
              git clone https://oauth2:${{ secrets.GIT_TOKEN }}@github.com/Zen-p/pet-project.git
              cd pet-project
            fi
            docker compose down -v
            docker compose pull
            docker compose up -d
```

Логика этого пайплайна хорошо иллюстрирует, как компоненты CI/CD взаимодействуют в реальной системе. В джобе `build` Docker‑логин происходит с использованием секретов GitHub, токен билда генерируется из текущей даты и времени, и передаётся как параметр `docker.version.tag` в Maven, чтобы Jib смог запушить образ с уникальным тегом в Docker Hub. Джоба `deploy-to-vps` через SSH подключается к удалённой машине, обновляет код, и затем использует `docker compose down -v`, `docker compose pull` и `docker compose up -d`, чтобы перезапустить стек микросервисов уже с новым образом.

Для продакшна принципиально важно, что деплой работает детерминированно: конкретный билд создаёт конкретный тег образа, этот тег прописывается в `docker-compose.yml` или в переменных окружения VPS, а затем деплой просто подтягивает нужную версию из реестра. Если нужно сделать rollback, достаточно переключиться на предыдущий тег и повторить процедуру `docker compose pull` и `docker compose up -d`.

## Принципы микросервисной архитектуры

Микросервисный подход в ваших заметках проявляется через набор специализированных сервисов и инфраструктурных компонентов: каждый микросервис имеет свой собственный Spring Boot main‑класс, собственный `pom.xml` и часто собственную базу данных. Вместо монолитного приложения, которое содержит весь функционал, вы строите систему из небольших автономных сервисов: сервис регистрации клиентов, антифрод‑сервис, сервис уведомлений, API‑шлюз, сервис‑реестр и т. д.

Главная идея такой архитектуры в том, что каждый сервис отвечает за узкую бизнес‑область и владеет своей моделью данных. Коммуникация между микросервисами происходит либо синхронно по HTTP (через RestTemplate или OpenFeign), либо асинхронно через брокеры сообщений вроде RabbitMQ и Kafka. В заметках это видно по конфигурациям RabbitMQ и Kafka, а также по примерам DTO, которые передаются между сервисами.

В реальной системе микросервисы должны быть слабо связаны и сильно когерентны. Слабая связность достигается за счёт того, что сервисы не знают напрямую о внутренней реализации друг друга: вместо этого они взаимодействуют через стабильные HTTP API или события в Kafka. Сильная когерентность — это когда код конкретного сервиса отражает единую предметную область: сервис антифрода не знает о логике регистрации пользователей, а сервис уведомлений не занимается валидацией транзакций.

Важным следствием разбиения на микросервисы является необходимость централизованного управления кросс‑сервисными аспектами: маршрутизация запросов, аутентификация, трейсинг, логирование и ограничение нагрузки. В заметках присутствуют решения для каждого из этих аспектов: Spring Cloud Gateway как API‑шлюз, Eureka как сервис‑реестр, Micrometer и Zipkin для трассировки запросов, RabbitMQ и Kafka для асинхронной обработки. Все эти компоненты запускаются в Docker‑контейнерах и координируются через Docker Compose или Kubernetes.

## Сервис‑реестр и Netflix Eureka

В распределённой системе, где каждый микросервис запускается в своём контейнере и может динамически масштабироваться, нельзя жёстко зашивать адреса вида `localhost:8081` в клиентский код. Вместо этого используется сервис‑реестр, в который сервисы регистрируют себя, а клиенты умеют находить их по логическому имени. В ваших заметках в этой роли выступает Netflix Eureka.

Сервер Eureka разворачивается как отдельный Spring Boot‑приложение с зависимостью `spring-cloud-starter-netflix-eureka-server` и конфигурацией порта.

```yaml
server:
  port: 8761

eureka:
  client:
    fetch-registry: false
    register-with-eureka: false
```

Здесь явно указано, что сервер Eureka сам не пытается ни с кем регистрироваться и не запрашивает чужие реестры. Он выступает корневой точкой, к которой обращаются все остальные микросервисы. В класс `@SpringBootApplication` добавляется аннотация `@EnableEurekaServer` (или эквивалентные конфигурации Spring Cloud), после чего по адресу `http://localhost:8761` становится доступна консоль, где можно увидеть список зарегистрированных экземпляров сервисов.

Клиентские микросервисы подключают зависимость `spring-cloud-starter-netflix-eureka-client` (или `spring-cloud-starter` с автоконфигурацией) и добавляют настройки, указывающие, где искать сервер.

```yaml
spring:
  application:
    name: CUSTOMER

eureka:
  client:
    service-url:
      defaultZone: http://localhost:8761/eureka
  instance:
    instance-id: ${spring.application.name}:${random.value}
```

Здесь `spring.application.name` определяет логическое имя сервиса, под которым он будет виден в Eureka. Поле `instance-id` делает каждый экземпляр уникальным, добавляя случайное значение. Это полезно при запуске нескольких инстансов одного и того же сервиса: вы сможете отличать их в интерфейсе Eureka. Как только клиентский сервис стартует, он регистрируется на `http://localhost:8761/eureka` и периодически посылает heartbeat, чтобы сообщать о своей живости.

Интересный эффект интеграции Eureka с остальной экосистемой Spring заключается в том, что после регистрации вы можете обращаться к сервису не через конкретный адрес и порт, а через логическое имя. В заметках это показано на примере RestTemplate: вместо `http://localhost:8080/api/v1/fraud-check/{customerId}` используется `http://FRAUD/api/v1/fraud-check/{customerId}`, а имя `FRAUD` резолвится через сервис‑реестр.

## Декларативные REST‑клиенты с OpenFeign

Хотя RestTemplate даёт полный контроль над HTTP‑запросами, в микросервисах он быстро приводит к громоздкому коду: ручное формирование URL, сериализация и десериализация JSON, обработка ошибок. OpenFeign, который вы используете в своих конспектах, предлагает декларативный подход: REST‑клиенты описываются интерфейсами, а Spring Cloud Feign генерирует реализацию за вас.

Для начала в `pom.xml` микросервиса добавляется зависимость.

```xml
<dependency>
  <groupId>org.springframework.cloud</groupId>
  <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
```

В основном классе приложения включается поддержка Feign‑клиентов, часто через `@EnableFeignClients(basePackages = "com.youdzhin.clients")`. Отдельные интерфейсы клиентов выносите в модуль `clients`, который затем подключается как зависимость к другим сервисам. Это правильный шаг, потому что описание удалённых API становится общим контрактом для всей системы.

Пример интерфейса Feign‑клиента для антифрод‑сервиса может выглядеть так.

```java
@FeignClient(
  name = "FRAUD",
  path = "/api/v1/fraud-check"
)
public interface FraudClient {

  @GetMapping(path = "/{customerId}")
  FraudCheckResponse isFraudster(@PathVariable("customerId") Long customerId);
}
```

Этот интерфейс описывает HTTP‑клиента к сервису `FRAUD`, зарегистрированному в Eureka. Атрибут `name` указывает логическое имя сервиса, `path` — общий префикс URL. Метод `isFraudster` помечен `@GetMapping` с шаблоном `/{customerId}` и принимает идентификатор клиента как `@PathVariable`. Возвращаемый тип `FraudCheckResponse` — это обычный DTO‑класс или `record`, который соответствует JSON‑ответу антифрод‑сервиса.

Как только такой интерфейс оказывается в пакете, сканируемом `@EnableFeignClients`, Spring Cloud автоматически генерирует реализацию. Для бизнес‑логики это означает, что вместо ручной работы с RestTemplate вы можете просто инжектировать `FraudClient` и вызывать у него метод `isFraudster`. Все детали построения URL, обращения к Eureka, балансировки нагрузки и сериализации/десериализации берёт на себя инфраструктура.

## Spring Cloud Gateway и шаблон API Gateway

API Gateway — это паттерн, который играет особо важную роль в продакшн‑микросервисах. Вместо того чтобы фронтенд или внешние клиенты напрямую ходили ко всем микросервисам, запросы стекаются на один шлюз, который выполняет маршрутизацию, авторизацию, rate limiting, логирование и многое другое. В ваших заметках API‑шлюз реализован с помощью Spring Cloud Gateway.

Базовая конфигурация шлюза в YAML может выглядеть так, как у вас в конспекте.

```yaml
spring:
  cloud:
    gateway:
      routes:
        - id: customer
          uri: lb://CUSTOMER
          predicates:
            - Path=/api/v1/customers/**
```

Здесь описан маршрут с идентификатором `customer`. Поле `uri: lb://CUSTOMER` говорит, что все запросы, подпадающие под условие `Path=/api/v1/customers/**`, должны быть направлены к сервису с логическим именем `CUSTOMER` через клиентскую балансировку нагрузки. Предикат `Path` определяет, какие URL‑пути обслуживаются этим маршрутом. В реальной системе таких маршрутов будет десятки: для авторизации, для заказов, для платежей и т. д.

Spring Cloud Gateway поддерживает и более продвинутый сценарий, который описан в вашем коде через `EurekaRouteDefinitionLocator`. Там маршруты генерируются динамически на основе информации из Eureka: для каждого зарегистрированного сервиса создаётся маршрут вида `/api/<service-name>/**`, при этом фильтр `RewritePath` удаляет префикс перед проксированием запроса к целевому сервису. Это позволяет добавлять новые микросервисы без ручного редактирования конфигурации шлюза: достаточно зарегистрировать сервис в Eureka, и он автоматически появится в маршрутах.

Кроме маршрутизации Gateway часто отвечает за кросс‑сервисную аутентификацию. Например, JWT‑токен, полученный пользователем при логине, проверяется на уровне шлюза, и только затем запросы пробрасываются к внутренним сервисам. Это снимает необходимость дублировать сложную конфигурацию безопасности в каждом микросервисе и упрощает сопровождение.

## Балансировка нагрузки и интеграция с Eureka

В распределённой системе не хватает просто знать, где находится сервис; нужно ещё и правильно распределять нагрузку между его инстансами. В заметках эта тема фигурирует через использование префикса `lb://` в Spring Cloud Gateway и аннотацию `@LoadBalanced` для RestTemplate. Обе эти конструкции включают в работу Spring Cloud LoadBalancer, который взаимодействует с Eureka.

Когда у вас, например, три инстанса сервиса `FRAUD`, зарегистрированных под разными `instance-id`, Eureka хранит их адреса и порты. RestTemplate с аннотацией `@LoadBalanced` и Feign‑клиенты при обращении к `http://FRAUD/...` не используют жёстко заданный `localhost:8081`, а получают список доступных инстансов через Eureka и выбирают один из них по заданной стратегии (чаще всего это round‑robin). Аналогично Spring Cloud Gateway, видя URI `lb://CUSTOMER`, обращается к сервис‑реестру и распределяет входящие запросы между всеми живыми инстансами `CUSTOMER`.

Концептуально это называется клиентской балансировкой: именно клиент (RestTemplate, Feign, Gateway) выбирает целевой инстанс, а не внешний балансировщик вроде Nginx. Такая схема хорошо масштабируется внутрь Kubernetes‑кластера или Docker‑сети и позволяет системе быть устойчивой к падению отдельных инстансов. Если один из контейнеров сервиса перестал отвечать и пропал из Eureka, новые запросы к нему больше не пойдут.

При проектировании микросервисов под балансировку нагрузки важно учитывать идемпотентность запросов и корректную работу при повторных вызовах. Например, в мире Kafka вы уже встречали идею «at least once» и требование обрабатывать дубликаты сообщений; в HTTP‑микросервисах аналогичный принцип применяется к операциям, которые могут быть повторно отправлены из‑за таймаутов или сбоев.

## Как все компоненты работают вместе в реальной системе

Если собрать все описанные выше элементы, получается типичная продакшн‑архитектура. Разработчик пишет код микросервиса на Spring Boot, описывает зависимости и плагины в `pom.xml`, включая `spring-boot-maven-plugin`, Jib и зависимости Spring Cloud. При коммите в GitHub запускается workflow CI, который поднимает PostgreSQL, собирает и тестирует проект. Для релизной ветки или при ручном запуске workflow CD Maven вместе с Jib строит Docker‑образ микросервиса, пушит его в Docker Hub с тегом, завязанным на build‑номер, после чего по SSH на VPS обновляется репозиторий и выполняется `docker compose pull` и `docker compose up -d`.

На VPS Docker Compose управляет множеством контейнеров: Eureka‑сервер, Zipkin, RabbitMQ, PostgreSQL, несколько бизнес‑микросервисов и API‑шлюз на Spring Cloud Gateway. Микросервисы регистрируются в Eureka, Gateway маршрутизирует все внешние запросы по имени сервиса, а Feign‑клиенты и RestTemplate с `@LoadBalanced` упрощают межсервисные вызовы. Асинхронные сценарии (например, отправка уведомлений) реализованы через RabbitMQ или Kafka, что видно по богатому набору конфигураций брокеров сообщений и топиков в ваших заметках. Для диагностики включены Micrometer, Zipkin и, при необходимости, Prometheus, что даёт полную картину прохождения запросов по системе.

## Отладка и типичные проблемы в микросервисной среде

При полном стеке микросервисов на Docker чаще всего возникают проблемы на стыке компонентов: сервис не может подключиться к базе данных, не видит Eureka, не попадает в нужный маршрут Gateway или не может найти Docker‑образ в реестре. Ваши конспекты уже содержат многие фрагменты, помогающие разбираться с такими ситуациями: использование `docker logs`, `kubectl logs`, конфигурации Zipkin и Micrometer, детальные настройки RabbitMQ и Kafka.

Первая категория проблем связана с сетью и именованием. Если локально вы привыкли использовать `localhost:5432`, то в Docker‑среде, где PostgreSQL запущен как сервис `postgres` в Docker Compose, строка подключения должна выглядеть как `jdbc:postgresql://postgres:5432/people`. То же самое относится к обращению к микросервисам: вместо `localhost:8081` нужно использовать логическое имя сервиса, если вы работаете через Eureka и Spring Cloud LoadBalancer. В случае неочевидных 404 или 503 от Gateway полезно проверить консоль Eureka, чтобы убедиться, что целевой сервис действительно зарегистрирован и в статусе UP.

Вторая частая проблема — порядок запуска и готовность зависимостей. Хотя `depends_on` в Docker Compose гарантирует, что контейнер базы данных стартует раньше, чем бизнес‑сервис, это ещё не значит, что база готова принимать подключения. В таких случаях помогает либо внутренняя логика повторных попыток в Spring Boot (например, с помощью `spring.datasource.initialization-mode` и retry‑механизмов), либо health‑checks на уровне Compose и Kubernetes, которые удерживают сервис в состоянии «неготов» до тех пор, пока он не сможет обслуживать запросы.

Третья категория касается несоответствия конфигураций. Выделение отдельных профилей, таких как `docker`, и явное указание `SPRING_PROFILES_ACTIVE` в Docker Compose помогают избежать ситуации, когда приложение в контейнере пытается подключиться к локальной базе или использовать путь к файлам, доступный только на машине разработчика. Для микросервисов с Feign‑клиентами важно убедиться, что `spring.application.name` совпадает с тем именем, которое используется в URI `lb://...` и в настройках Eureka; любое расхождение приводит к тому, что клиентский код просто не может найти сервис в реестре.

Четвёртая область отладки — задержки и таймауты при межсервисных вызовах. Здесь очень полезны те механизмы трассировки, которые вы уже упоминали: Zipkin, Micrometer, Micrometer Tracing. Подключив их через зависимости `micrometer-tracing-bridge-otel` и экспортёр Zipkin, вы получаете наглядные трейс‑диаграммы, показывающие, какой из микросервисов в цепочке отвечает дольше всего. Это позволяет быстро локализовать «узкое место», будь то медленный SQL‑запрос в одном сервисе или блокировка в Kafka‑консьюмере.

Наконец, в продакшн‑окружении важно предусмотреть поведение системы при частичных отказах. Если один из микросервисов временно недоступен, Eureka и Spring Cloud Gateway перестанут направлять к нему запросы, но вызывающая сторона должна быть готова к ошибкам и уметь их обрабатывать: использовать retry, circuit breaker, возвращать пользовательские сообщения об ошибках. В ваших заметках уже заложена хорошая основа в виде централизованной обработки ошибок через `@ResponseStatus` и кастомные исключения, а также использования брокеров сообщений — они позволяют сгладить пики нагрузки и повысить устойчивость системы.

В совокупности Docker, Docker Compose, Jib, GitHub Actions, Eureka, OpenFeign, Spring Cloud Gateway и механизмы балансировки нагрузки образуют законченную экосистему для построения продакшн‑готовых микросервисных архитектур. Ваши заметки уже содержат почти все необходимые кирпичики; задача этой лекции — связать их в единую картину и показать, как каждый из компонентов вписывается в реальную систему, от коммита до распределённого продакшн‑кластера.


